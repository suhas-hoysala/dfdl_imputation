{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 12:35:26.314144: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-15 12:35:26.316194: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-15 12:35:26.358569: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-15 12:35:26.359471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-15 12:35:27.221951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 12:35:28.221373: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from GENIE3.GENIE3 import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import gt_benchmark, precision_at_k\n",
    "import SERGIO.sergio as sergio\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"gpu available: \", tf.config.list_physical_devices('GPU'))\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_name(folder_name):\n",
    "    pattern1 = r'De-noised_(\\d+)G_(\\d+)T_(\\d+)cPerT_dynamics_(\\d+)_DS(\\d+)'\n",
    "    pattern2 = r'De-noised_(\\d+)G_(\\d+)T_(\\d+)cPerT_(\\d+)_DS(\\d+)'\n",
    "    match_p1 = re.match(pattern1, folder_name)\n",
    "    match_p2 = re.match(pattern2, folder_name)\n",
    "    if match_p1:\n",
    "        return {\n",
    "            'number_genes': int(match_p1.group(1)),\n",
    "            'number_bins': int(match_p1.group(2)),\n",
    "            'number_sc': int(match_p1.group(3)),\n",
    "            'dynamics': int(match_p1.group(4)),\n",
    "            'dataset_id': int(match_p1.group(5)),\n",
    "            \"pattern\": \"De-noised_{number_genes}G_{number_bins}T_{number_sc}cPerT_dynamics_{dynamics}_DS{dataset_id}\"\n",
    "        }\n",
    "    if match_p2:\n",
    "        return {\n",
    "            'number_genes': int(match_p2.group(1)),\n",
    "            'number_bins': int(match_p2.group(2)),\n",
    "            'number_sc': int(match_p2.group(3)),\n",
    "            'dynamics': int(match_p2.group(4)),\n",
    "            'dataset_id': int(match_p2.group(5)),\n",
    "            \"pattern\": \"De-noised_{number_genes}G_{number_bins}T_{number_sc}cPerT_{dynamics}_DS{dataset_id}\"\n",
    "        }\n",
    "    return\n",
    "\n",
    "def get_datasets():\n",
    "    datasets = []\n",
    "    for folder_name in os.listdir('./data_sets'):\n",
    "        dataset_info = parse_dataset_name(folder_name)\n",
    "        if dataset_info:\n",
    "            datasets.append(dataset_info)\n",
    "    return sorted(datasets, key=lambda x: x['dataset_id'])\n",
    "\n",
    "def fstr(template):\n",
    "    return eval(f'f\"\"\"{template}\"\"\"')\n",
    "\n",
    "def experiment(data_info):\n",
    "    sim = sergio.sergio(\n",
    "        number_genes=data_info[\"number_genes\"],\n",
    "        number_bins=data_info[\"number_bins\"], \n",
    "        number_sc=data_info[\"number_sc\"],\n",
    "        noise_params=1,\n",
    "        decays=0.8, \n",
    "        sampling_state=15,\n",
    "        noise_type='dpd'\n",
    "    )\n",
    "    # sim.build_graph(input_file_taregts ='data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Interaction_cID_6.txt',\\\n",
    "    #                 input_file_regs='data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Regs_cID_6.txt', shared_coop_state=2)\n",
    "    number_genes = data_info[\"number_genes\"]\n",
    "    number_bins = data_info[\"number_bins\"]\n",
    "    number_sc = data_info[\"number_sc\"]\n",
    "    dynamics = data_info[\"dynamics\"]\n",
    "    dataset_id = data_info[\"dataset_id\"]\n",
    "    pattern = data_info[\"pattern\"]\n",
    "    folder_name = pattern.format(number_genes=number_genes, number_bins=number_bins, \n",
    "                                 number_sc=number_sc, dynamics=dynamics, dataset_id=dataset_id)\n",
    "    input_file_targets = f'./data_sets/{folder_name}/Interaction_cID_{data_info[\"dynamics\"]}.txt'\n",
    "    input_file_regs = f'./data_sets/{folder_name}/Regs_cID_{data_info[\"dynamics\"]}.txt'\n",
    "    \n",
    "    sim.build_graph(\n",
    "        input_file_taregts=input_file_targets,\n",
    "        input_file_regs=input_file_regs,\n",
    "        shared_coop_state=2\n",
    "    )\n",
    "    sim.simulate()\n",
    "    expr = sim.getExpressions()\n",
    "    expr_clean = np.concatenate(expr, axis=1)\n",
    "    return sim, expr, expr_clean\n",
    "\n",
    "def save_data(dataset_id, expr_clean, expr, sim, iter=0):\n",
    "    print(f\"DS{dataset_id}: {expr_clean.shape}\")\n",
    "    os.makedirs('./imputation_data/DS{dataset_id}', exist_ok=True)\n",
    "    np.save(f'./imputation_data/DS{dataset_id}/DS6_clean_iter_{iter}', expr_clean)\n",
    "    np.save(f'./imputation_data/DS{dataset_id}/DS6_expr_iter_{iter}', expr)\n",
    "    cmat_clean = sim.convert_to_UMIcounts(expr)\n",
    "    cmat_clean = np.concatenate(cmat_clean, axis=1)\n",
    "    np.save(f'./imputation_data/DS{dataset_id}/DS6_clean_counts_iter_{iter}', cmat_clean)\n",
    "\n",
    "def sparse_ratio(data):\n",
    "    # ndarray\n",
    "    return 1 - np.count_nonzero(data) / data.size\n",
    "\n",
    "def get_sparsity_of_binary_ind(sim, expr, expr_clean, percentile=45, dataset_id=6, iter=0):\n",
    "    \"\"\"\n",
    "    Add outlier genes\n",
    "    \"\"\"\n",
    "    expr_O = sim.outlier_effect(expr, outlier_prob = 0.01, mean = 5, scale = 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Add Library Size Effect\n",
    "    \"\"\"\n",
    "    libFactor, expr_O_L = sim.lib_size_effect(expr_O, mean = 4.5, scale = 0.7)\n",
    "\n",
    "    \"\"\"\n",
    "    Add Dropouts\n",
    "    \"\"\"\n",
    "    binary_ind = sim.dropout_indicator(expr_O_L, shape = 8, percentile = percentile)\n",
    "    expr_O_L_D = np.multiply(binary_ind, expr_O_L)\n",
    "\n",
    "    \"\"\"\n",
    "    Convert to UMI count\n",
    "    \"\"\"\n",
    "    count_matrix = sim.convert_to_UMIcounts(expr_O_L_D)\n",
    "\n",
    "    \"\"\"\n",
    "    Make a 2d gene expression matrix\n",
    "    \"\"\"\n",
    "    count_matrix = np.concatenate(count_matrix, axis = 1)\n",
    "    os.makedirs(os.path.dirname(f'./imputation_data/DS{dataset_id}/DS6_45'), exist_ok=True)\n",
    "    np.save(f'./imputation_data/DS{dataset_id}/DS6_45_iter_{iter}', count_matrix)\n",
    "    print(count_matrix.shape)\n",
    "    return sparse_ratio(binary_ind), expr_O, libFactor, expr_O_L, binary_ind, count_matrix\n",
    "\n",
    "def compute_checksum(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return hashlib.md5(data.tobytes()).hexdigest()\n",
    "    elif isinstance(data, list):\n",
    "        return hashlib.md5(str(data).encode()).hexdigest()\n",
    "    else:\n",
    "        return hashlib.md5(str(data).encode()).hexdigest()\n",
    "\n",
    "def compute_stats(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return {\n",
    "            'checksum': compute_checksum(data),\n",
    "            'mean': np.mean(data),\n",
    "            'std': np.std(data)\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return {\n",
    "            'checksum': compute_checksum(data),\n",
    "            'mean': np.mean(data),\n",
    "            'std': np.std(data)\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'checksum': compute_checksum(data),\n",
    "            'value': data\n",
    "        }\n",
    "\n",
    "def compare_attempts(attempts):\n",
    "    comparison = {}\n",
    "    for key in attempts[0].keys():\n",
    "        values = [attempt[key]['checksum'] for attempt in attempts]\n",
    "        comparison[key] = len(set(values)) > 1\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def run_exp1(x_path, y_path, ind):\n",
    "    #reload_modules('tensorflow.compat')\n",
    "    tf = importlib.import_module('tensorflow.compat.v1')\n",
    "    ds_str = 'DS' + str(ind)\n",
    "    save_path = './imputation_data/' + ds_str\n",
    "    print(f\"Loading data for DS{ind}\")\n",
    "    x = np.transpose(np.load(x_path))  # Clean data\n",
    "    y = np.transpose(np.load(y_path))  # Noisy data\n",
    "    # get num of cluters\n",
    "    n_clusters = get_num_cell_types(y, cells_per_cluster=300)\n",
    "    # perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(y)\n",
    "\n",
    "    print(f\"Running Exp1 imputation for DS{ind}\")\n",
    "    imputed_y = np.copy(y)\n",
    "    for cluster in np.unique(clusters):\n",
    "        cluster_indices = np.where(clusters == cluster)[0]\n",
    "        cluster_data = imputed_y[cluster_indices, :]\n",
    "        for gene_idx in range(cluster_data.shape[1]):\n",
    "            gene_values = cluster_data[:, gene_idx]\n",
    "            non_zero_values = gene_values[gene_values != 0] # exclude missing values\n",
    "            if len(non_zero_values) > 0:\n",
    "                mean_gene = np.mean(non_zero_values)\n",
    "                std_gene = np.std(non_zero_values)\n",
    "                missing_indices = np.where(gene_values == 0)[0] # zeros are missing values\n",
    "                imputed_values = np.random.normal(mean_gene, std_gene, size=missing_indices.shape[0])\n",
    "                imputed_y[cluster_indices[missing_indices], gene_idx] = imputed_values\n",
    "\n",
    "    print(f\"Saving imputed data for DS{ind}\")\n",
    "    save_str = '/yhat_exp1'\n",
    "    np.save(save_path + save_str, imputed_y)\n",
    "    print(f\"Exp1 imputation completed for DS{ind} and results saved.\")\n",
    "\n",
    "def run_exp1_wo_cluster(x_path, y_path, ind):\n",
    "    ds_str = 'DS' + str(ind)\n",
    "    save_path = './imputation_data/' + ds_str\n",
    "\n",
    "    # load clean and noisy data\n",
    "    print(f\"Loading data for DS{ind}\")\n",
    "    x_clean = np.transpose(np.load(x_path))\n",
    "    y_noisy = np.transpose(np.load(y_path))\n",
    "    total_rows = y_noisy.shape[0]\n",
    "    num_blocks = total_rows // 300\n",
    "    block_means = np.zeros((num_blocks, y_noisy.shape[1]))\n",
    "    block_variances = np.zeros((num_blocks, y_noisy.shape[1]))\n",
    "\n",
    "    imputed_y = np.copy(y_noisy)\n",
    "    intra_block_mse = []\n",
    "    inter_block_mse = []\n",
    "\n",
    "    for block in range(num_blocks):\n",
    "        start_row = block * 300\n",
    "        end_row = start_row + 300\n",
    "\n",
    "        # verify the means of each row in the block\n",
    "        print(f\"Verifying means and variances of rows {start_row} to {end_row} in clean data\")\n",
    "        block_means[block, :] = np.mean(x_clean[start_row:end_row, :], axis=0)\n",
    "        # print(f\"Means of genes in block {block + 1}: {block_means[block, :]}\")\n",
    "        block_variances[block, :] = np.var(x_clean[start_row:end_row, :], axis=0)\n",
    "        # print(f\"Variances of genes in block {block + 1}: {block_variances[block, :]}\")\n",
    "\n",
    "        # verify means of 300 rows in clean data\n",
    "        set_means = []\n",
    "        for i in range(5):\n",
    "            subset = x_clean[start_row + i*60: start_row + (i+1)*60, :]\n",
    "            mean_subset = np.mean(subset, axis=0)\n",
    "            set_means.append(mean_subset)\n",
    "            # print(f\"Mean of set {i+1} in block {block + 1}: {mean_subset}\")\n",
    "\n",
    "        # Intra-block MSE, compare mean vectors of sets within the same block\n",
    "        for i in range(5):\n",
    "            for j in range(i + 1, 5):\n",
    "                mse = mean_squared_error(set_means[i], set_means[j])\n",
    "                intra_block_mse.append(mse)\n",
    "                print(f\"Intra-block MSE between set {i+1} and set {j+1} in block {block + 1}: {mse}\")\n",
    "        \n",
    "        # Inter-block MSE, compare mean vectors of this block with the next block\n",
    "        if block < num_blocks - 1:\n",
    "            next_block_start = (block + 1) * 300\n",
    "            next_block_end = next_block_start + 300\n",
    "            next_block_means = []\n",
    "            for i in range(5):\n",
    "                next_subset = x_clean[next_block_start + i*60: next_block_start + (i+1)*60, :]\n",
    "                next_mean_subset = np.mean(next_subset, axis=0)\n",
    "                next_block_means.append(next_mean_subset)\n",
    "\n",
    "            for i in range(5):\n",
    "                mse = mean_squared_error(set_means[i], next_block_means[i])\n",
    "                inter_block_mse.append(mse)\n",
    "                print(f\"Inter-block MSE between set {i+1} of block {block + 1} and set {i+1} of block {block + 2}: {mse}\")\n",
    "        \n",
    "        for i in range(5):\n",
    "            subset = x_clean[start_row + i*60 : start_row + (i+1)*60, :]\n",
    "            # Initialize arrays to store means and variances for each column\n",
    "            mean_subset = np.zeros(subset.shape[1])\n",
    "            variance_subset = np.zeros(subset.shape[1])\n",
    "            for col_idx in range(subset.shape[1]):\n",
    "                non_zero_values = subset[:, col_idx][subset[:, col_idx] != 0]\n",
    "                if len(non_zero_values) > 0:\n",
    "                    mean_subset[col_idx] = np.mean(non_zero_values) \n",
    "                    variance_subset[col_idx] = np.var(non_zero_values)\n",
    "            # print(f\"Mean of set {i+1} in block {block + 1}: {mean_subset}, Variance: {variance_subset}\")\n",
    "       \n",
    "        print(f\"Total zeros in block {block + 1} before imputation: {np.sum(y_noisy[start_row:end_row, :] == 0)}\")\n",
    "        # compute mean and variance for each gene from the noisy data for each 300 rows\n",
    "        print(f\"Computing mean and variance from the noisy data for rows {start_row} to {end_row}\")\n",
    "        y_300 = y_noisy[start_row:end_row, :]\n",
    "        means = np.zeros(y_300.shape[1])\n",
    "        variances = np.zeros(y_300.shape[1])\n",
    "        min_val = np.zeros(y_300.shape[1])\n",
    "        max_val = np.zeros(y_300.shape[1])\n",
    "        for gene_idx in range(y_300.shape[1]):\n",
    "            non_zero_values = y_300[:, gene_idx][y_300[:, gene_idx] != 0]\n",
    "            # print(f\"Gene {gene_idx} has {len(non_zero_values)} non-zero values\")\n",
    "            if len(non_zero_values) > 0:\n",
    "                means[gene_idx] = np.mean(non_zero_values)\n",
    "                variances[gene_idx] = np.var(non_zero_values)\n",
    "                min_val[gene_idx] = np.min(non_zero_values)\n",
    "                max_val[gene_idx] = np.max(non_zero_values)\n",
    "        # imputation by fill in zeros with normal distribution\n",
    "        print(f\"Imputing missing values (zeros) for rows {start_row} to {end_row}\")\n",
    "        block_imputed_y = np.copy(y_noisy[start_row:end_row, :])\n",
    "        for gene_idx in range(block_imputed_y.shape[1]):\n",
    "            missing_indices = np.where(block_imputed_y[:, gene_idx] == 0)[0]\n",
    "            # print(f\"Gene {gene_idx} has {len(missing_indices)} missing values\")\n",
    "            if len(missing_indices) > 0:\n",
    "                imputed_values = np.random.normal(means[gene_idx], np.sqrt(variances[gene_idx]), size=len(missing_indices))\n",
    "                imputed_values = np.clip(imputed_values, min_val[gene_idx], max_val[gene_idx])\n",
    "                print(f\"Imputed values for gene {gene_idx}: min={np.min(imputed_values)}, max={np.max(imputed_values)}\")\n",
    "                block_imputed_y[missing_indices, gene_idx] = imputed_values\n",
    "\n",
    "        if np.any(block_imputed_y == 0):\n",
    "            print(f\"Warning: Block {block + 1} still contains zero values after imputation!\")\n",
    "\n",
    "        imputed_y[start_row:end_row, :] = block_imputed_y\n",
    "\n",
    "    print(f\"imputed y in the method is: {imputed_y}\")\n",
    "    print(f\"Saving final imputed data for DS{ind}\")\n",
    "    save_str = '/yhat_exp1.npy'\n",
    "    np.save(save_path + save_str, imputed_y)\n",
    "    print(f\"Exp1 imputation completed for DS{ind} and final dataset saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_cell_types(y, cells_per_cluster):\n",
    "    total_cells = y.shape[0]\n",
    "    n_clusters = max(1, total_cells // cells_per_cluster)\n",
    "    return n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Running Exp1 on DS1\n",
      "Loading data for DS1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './imputation_data/DS1/DS6_clean_iter_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# TODO: change iter num\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# run_exp1(save_path + f'/DS6_clean_iter_{iter_num}.npy', save_path + f'/DS6_45_iter_{iter_num}.npy', i)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mrun_exp1_wo_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/DS6_clean_iter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miter_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/DS6_45_iter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miter_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# load nosiy and clean data\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mload(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/DS6_45_iter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mrun_exp1_wo_cluster\u001b[0;34m(x_path, y_path, ind)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# load clean and noisy data\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data for DS\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m x_clean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     45\u001b[0m y_noisy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mload(y_path))\n\u001b[1;32m     46\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m y_noisy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/scratch/yz5944/miniconda3/envs/bio2/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './imputation_data/DS1/DS6_clean_iter_0.npy'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import json\n",
    "\n",
    "datasets = get_datasets()\n",
    "for k, dataset in tqdm(enumerate(datasets)):\n",
    "    i = k + 1\n",
    "    #  dataset info\n",
    "    dataset_id = dataset['dataset_id']\n",
    "    number_genes = dataset[\"number_genes\"]\n",
    "    number_bins = dataset[\"number_bins\"]\n",
    "    number_sc = dataset[\"number_sc\"]\n",
    "    dynamics = dataset[\"dynamics\"]\n",
    "    dataset_id = dataset[\"dataset_id\"]\n",
    "    pattern = dataset[\"pattern\"]\n",
    "    folder_name = pattern.format(number_genes=number_genes, number_bins=number_bins, \n",
    "                                 number_sc=number_sc, dynamics=dynamics, dataset_id=dataset_id)\n",
    "    target_file = f'./data_sets/{folder_name}/Interaction_cID_{dataset[\"dynamics\"]}.txt'\n",
    "    input_file = f'./data_sets/{folder_name}/Regs_cID_{dataset[\"dynamics\"]}.txt'\n",
    "\n",
    "    # run SERGIO\n",
    "    # sim, expr, expr_clean = experiment(dataset)\n",
    "    # ratio, expr_O, libFactor, expr_O_L, binary_ind, count_matrix = get_sparsity_of_binary_ind(sim, expr, expr_clean, percentile=percentile, dataset_id=dataset_id)\n",
    "    \n",
    "    # prepare genie3\n",
    "    individual_results = {}\n",
    "    if i == 1:   \n",
    "        target_file = './data_sets/De-noised_100G_9T_300cPerT_4_DS1/Interaction_cID_4.txt'\n",
    "        regs_path = './data_sets/De-noised_100G_9T_300cPerT_4_DS1/Regs_cID_4.txt'\n",
    "    elif i == 2:\n",
    "        target_file = './data_sets/De-noised_400G_9T_300cPerT_5_DS2/Interaction_cID_5.txt'\n",
    "        regs_path = './data_sets/De-noised_400G_9T_300cPerT_5_DS2/Regs_cID_5.txt'\n",
    "    else:\n",
    "        target_file = './data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Interaction_cID_6.txt'\n",
    "        regs_path = './data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Regs_cID_6.txt'\n",
    "    ds_str = 'DS' + str(i)\n",
    "    save_path = './imputation_data/' + ds_str\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # run Exp1 imputation method\n",
    "    print(f\"---> Running Exp1 on DS{i}\")\n",
    "    # TODO: change iter num\n",
    "    for iter_num in range(1):\n",
    "        # run_exp1(save_path + f'/DS6_clean_iter_{iter_num}.npy', save_path + f'/DS6_45_iter_{iter_num}.npy', i)\n",
    "        run_exp1_wo_cluster(save_path + f'/DS6_clean_iter_{iter_num}.npy', save_path + f'/DS6_45_iter_{iter_num}.npy', i)\n",
    "\n",
    "        # load nosiy and clean data\n",
    "        y = np.transpose(np.load(save_path + f'/DS6_45_iter_{iter_num}.npy'))\n",
    "        print(\"what is y?: \", y)\n",
    "        x = np.transpose(np.load(save_path + f'/DS6_clean_iter_{iter_num}.npy'))\n",
    "        \n",
    "        # get true regulator genes from SERGIO data\n",
    "        reg_file = None\n",
    "        if i == 1:\n",
    "            reg_file = './data_sets/De-noised_100G_9T_300cPerT_4_DS1/Regs_cID_4.txt'\n",
    "        elif i == 2:\n",
    "            reg_file = './data_sets/De-noised_400G_9T_300cPerT_5_DS2/Regs_cID_5.txt'\n",
    "        else:\n",
    "            reg_file = './data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Regs_cID_6.txt'\n",
    "        master_regs = pd.read_table(reg_file, header=None, sep=',')\n",
    "        master_regs = master_regs[0].values.astype(int).astype(str).tolist()\n",
    "\n",
    "        regulators = []\n",
    "        regulator_file = open(target_file, 'r')\n",
    "        lines = regulator_file.readlines()\n",
    "        for line in lines:\n",
    "            row = line.split(',')\n",
    "            num_regs_row = int(float(row[1]))\n",
    "            if num_regs_row != 0:\n",
    "                for i in range(2, num_regs_row + 2):\n",
    "                    regulators.append(str(int(float(row[i]))))\n",
    "        regs = list(set(regulators))\n",
    "        regs = [i for i in regs if i not in master_regs]\n",
    "\n",
    "        # Run GENIE3 on Clean Data\n",
    "        print(f\"---> Running GENIE3 on Clean Data for DS{i}\")\n",
    "        gene_names = [str(i) for i in range(x.shape[1])]\n",
    "        # if not run_with_regs:\n",
    "        regs = None\n",
    "        gene_names = None\n",
    "\n",
    "        VIM_CLEAN = GENIE3(x, nthreads=12, ntrees=100)        \n",
    "        gt, rescaled_vim = gt_benchmark(VIM_CLEAN, target_file)\n",
    "        # Aucroc\n",
    "        roc_score = roc_auc_score(gt.flatten(), rescaled_vim.flatten())\n",
    "        individual_results['DS' + str(i) + ' GENIE3 Clean ROC_AUC'] = float('%.2f'%(roc_score))\n",
    "        # precision k\n",
    "        # k = range(1, gt.size)\n",
    "        # precision_k = precision_at_k(gt, rescaled_vim, k)\n",
    "        # individual_results['DS' + str(i) + ' GENIE3 Clean Precision@k'] = precision_k\n",
    "\n",
    "        # Run GENIE3 on Noisy Data\n",
    "        print(f\"---> Running GENIE3 on Noisy Data for DS{i}\")\n",
    "        gene_names = [str(i) for i in range(y.shape[1])]\n",
    "        VIM_NOISY = GENIE3(y, nthreads=12, ntrees=100)       \n",
    "        gt, rescaled_vim = gt_benchmark(VIM_NOISY, target_file)\n",
    "        # Aucroc\n",
    "        roc_score = roc_auc_score(gt.flatten(), rescaled_vim.flatten())\n",
    "        individual_results['DS' + str(i) + ' GENIE3 Noisy ROC_AUC'] = float('%.2f'%(roc_score))\n",
    "        # precision k\n",
    "        # k = range(1, gt.size)\n",
    "        # precision_k = precision_at_k(gt, rescaled_vim, k)\n",
    "        # individual_results['DS' + str(i) + ' GENIE3 Noisy Precision@k'] = precision_k\n",
    "\n",
    "        # Run GENIE3 on Exp1 Data, basic imputation method, no clustering, just fill in zeros with normal distribution and clip the distribution with non-zero data\n",
    "        y_hat_exp1 = np.load(save_path + '/yhat_exp1.npy')\n",
    "        print(y_hat_exp1)\n",
    "        print(f\"---> Running GENIE3 on exp1 Data for DS{i}\")\n",
    "        gene_names = [str(i) for i in range(y_hat_exp1.shape[1])]\n",
    "        VIM_exp1 = GENIE3(y_hat_exp1, nthreads=12, ntrees=100)\n",
    "        gt, rescaled_vim = gt_benchmark(VIM_exp1, target_file)\n",
    "        np.save(save_path + '/VIM_exp1.npy', rescaled_vim)\n",
    "        np.save(save_path + '/gt_exp1.npy', gt)\n",
    "        print(\"saved exp1 files\")\n",
    "        # Aucroc\n",
    "        roc_score = roc_auc_score(gt.flatten(), rescaled_vim.flatten())\n",
    "        individual_results['DS' + str(i) + ' GENIE3 exp1 ROC_AUC'] = float('%.2f'%(roc_score))\n",
    "        # precision k\n",
    "        # k = range(1, gt.size)\n",
    "        # precision_k = precision_at_k(gt, rescaled_vim, k)\n",
    "        # individual_results['DS' + str(i) + ' GENIE3 exp1 Precision@k'] = precision_k\n",
    "        \n",
    "        # write individual results to JSON file\n",
    "        if os.path.exists(save_path + '/precision_recall_data.json'):\n",
    "            with open(save_path + '/precision_recall_data.json', 'r') as fp:\n",
    "                existing_data = json.load(fp)\n",
    "            if not isinstance(existing_data, list):\n",
    "                existing_data = [existing_data]\n",
    "        else:\n",
    "            existing_data = []\n",
    "        existing_data.append(individual_results)\n",
    "        with open(save_path + '/precision_recall_data.json', 'w') as fp:\n",
    "            json.dump(existing_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DS3 GENIE3 Clean ROC_AUC': 0.69, 'DS3 GENIE3 Noisy ROC_AUC': 0.46, 'DS3 GENIE3 exp1 ROC_AUC': 0.44}\n"
     ]
    }
   ],
   "source": [
    "print(individual_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'DS3 GENIE3 Clean ROC_AUC': 0.7, 'DS3 GENIE3 Noisy ROC_AUC': 0.47, 'DS3 GENIE3 exp1 ROC_AUC': 0.46}\n",
    "{'DS3 GENIE3 Clean ROC_AUC': 0.7, 'DS3 GENIE3 Noisy ROC_AUC': 0.46, 'DS3 GENIE3 exp1 ROC_AUC': 0.42}\n",
    "{'DS3 GENIE3 Clean ROC_AUC': 0.72, 'DS3 GENIE3 Noisy ROC_AUC': 0.49, 'DS3 GENIE3 exp1 ROC_AUC': 0.46}\n",
    "{'DS3 GENIE3 Clean ROC_AUC': 0.7, 'DS3 GENIE3 Noisy ROC_AUC': 0.48, 'DS3 GENIE3 exp1 ROC_AUC': 0.48}\n",
    "{'DS3 GENIE3 Clean ROC_AUC': 0.72, 'DS3 GENIE3 Noisy ROC_AUC': 0.47, 'DS3 GENIE3 exp1 ROC_AUC': 0.45}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(y_hat_exp1 == 0):\n",
    "    print(f\"DS{i} contains missing or zero data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat_exp1)\n",
    "zero_count = np.count_nonzero(y_hat_exp1 == 0)\n",
    "print(f\"Number of zeros in y_hat_exp1: {zero_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.transpose(np.load('../previous_imputations/DS6_clean_iter_0.npy'))\n",
    "print(x)\n",
    "if np.any(x <= 0):\n",
    "    print(f\"x contains missing or zero data\")\n",
    "np.min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  2  0 ...  0  0  0]\n",
      "  [ 1  2  0 ...  0  2  4]\n",
      "  [ 1  2  0 ...  1  2  1]\n",
      "  ...\n",
      "  [ 1  3  0 ...  0  0  3]\n",
      "  [ 3  5  0 ...  1  7  4]\n",
      "  [ 3  0  0 ...  0  0  3]]\n",
      "\n",
      " [[ 0  4  4 ...  0  0  1]\n",
      "  [ 2  1  2 ...  2  0  0]\n",
      "  [ 0  1  3 ...  1  0  0]\n",
      "  ...\n",
      "  [ 1  0  3 ...  3  0  0]\n",
      "  [ 1  2  8 ...  7  0  0]\n",
      "  [ 0  0  3 ...  3  1  0]]\n",
      "\n",
      " [[ 3  3  0 ...  0  2  7]\n",
      "  [ 1  0  0 ...  0  2  9]\n",
      "  [ 0  0  0 ...  0  0  4]\n",
      "  ...\n",
      "  [ 1  1  0 ...  1  2  5]\n",
      "  [ 2  4  0 ...  1  6 11]\n",
      "  [ 3  0  1 ...  0  0  2]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0  3  0 ...  0  3  0]\n",
      "  [ 1  0  1 ...  0  7  0]\n",
      "  [ 0  0  0 ...  1  3  1]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  3  3]\n",
      "  [ 1  1  1 ...  3  3  2]\n",
      "  [ 1  0  0 ...  0  2  0]]\n",
      "\n",
      " [[ 7  0  0 ...  0  2  1]\n",
      "  [ 1  2  1 ...  0  6  0]\n",
      "  [ 0  1  0 ...  0  2  0]\n",
      "  ...\n",
      "  [ 0  1  3 ...  2  2  0]\n",
      "  [ 7  1  5 ...  5  3  5]\n",
      "  [ 7  1  2 ...  1  1  3]]\n",
      "\n",
      " [[ 0  1  0 ...  0  0  1]\n",
      "  [ 1  2  1 ...  2  2  3]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 2  3  2 ...  3  1  3]\n",
      "  [ 0  2  0 ...  2  4  9]\n",
      "  [ 4  0  0 ...  1  0  6]]]\n",
      "Data type of clean data: int64\n",
      "(300, 100, 9)\n",
      "[[0 5 0 ... 0 8 2]\n",
      " [2 0 0 ... 0 2 2]\n",
      " [2 3 1 ... 0 2 0]\n",
      " ...\n",
      " [0 2 1 ... 0 2 0]\n",
      " [1 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 1 1 1]]\n",
      "Data type of clean data: int64\n",
      "(2700, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.transpose(np.load('./imputation_data/DS1/DS6_45_iter_0.npy'))\n",
    "print(x1)\n",
    "print(f\"Data type of clean data: {x1.dtype}\")\n",
    "print(x1.shape)\n",
    "np.min(x1)\n",
    "\n",
    "x2 = np.transpose(np.load('./imputation_data/DS1/DS6_45.npy'))\n",
    "print(x2)\n",
    "print(f\"Data type of clean data: {x2.dtype}\")\n",
    "print(x2.shape)\n",
    "np.min(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('./HU_0270_Adipose_GSE136229_gene_count.h5', 'r') as f:\n",
    "    # Access the 'matrix' group\n",
    "    matrix_group = f['matrix']\n",
    "    \n",
    "    data = matrix_group['data'][:]\n",
    "    print(f\"Data type of 'data': {data.dtype}\")\n",
    "    print(data, len(data))\n",
    "    \n",
    "    data = matrix_group['indices'][:]\n",
    "    print(f\"Data type of 'indices': {data.dtype}\")\n",
    "    print(data)\n",
    "    \n",
    "    data = matrix_group['indptr'][:]\n",
    "    print(f\"Data type of 'indptr': {data.dtype}\")\n",
    "    print(data)\n",
    "    \n",
    "    data = matrix_group['shape'][:]\n",
    "    print(f\"Data type of 'shape': {data.dtype}\")\n",
    "    print(data)\n",
    "    \n",
    "    data = matrix_group['barcodes'][:]\n",
    "    print(f\"Data type of 'barcodes': {data.dtype}\")\n",
    "    print(data)\n",
    "    \n",
    "    data = matrix_group['features'][:]\n",
    "    print(f\"Data type of 'features': {data.dtype}\")\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
