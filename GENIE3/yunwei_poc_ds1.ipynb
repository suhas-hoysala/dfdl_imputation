{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 20:14:08.267891: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-18 20:14:09.304762: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-18 20:14:10.809903: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-18 20:14:10.810902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-18 20:14:25.057384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/yz5944/miniconda3/envs/bio2/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Import GENIE3\n",
    "from GENIE3.GENIE3 import GENIE3\n",
    "import baselines.scScope.scscope.scscope as scScope\n",
    "from baselines.SAUCIE.SAUCIE import SAUCIE, Loader\n",
    "from baselines.MAGIC.magic import magic\n",
    "\n",
    "# Import imputation methods\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# For deep learning-based imputation (DeepImpute)\n",
    "# !pip install deepimpute\n",
    "from baselines.deepimpute.deepimpute.multinet import MultiNet\n",
    "\n",
    "# For graph convolutional networks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from scipy.sparse.linalg import bicgstab\n",
    "import scprep\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "def load_data():\n",
    "    ds1_clean = np.load('./SERGIO/imputation_data/DS1/iterations_seperate/DS6_clean_iter_0.npy').astype(np.float32)\n",
    "    ds1_noisy = np.load('./SERGIO/imputation_data/DS1/iterations_seperate/DS6_45_iter_0.npy').astype(np.float32)\n",
    "    return ds1_clean, ds1_noisy\n",
    "\n",
    "# Load ground truth network\n",
    "def load_ground_truth(target_file, num_genes):\n",
    "    gt = np.zeros((num_genes, num_genes))\n",
    "    with open(target_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line_list = line.strip().split(',')\n",
    "        target_index = int(float(line_list[0]))\n",
    "        num_regs = int(float(line_list[1]))\n",
    "        for i in range(num_regs):\n",
    "            reg_index = int(float(line_list[i + 2]))\n",
    "            gt[reg_index, target_index] = 1\n",
    "    return gt\n",
    "\n",
    "# Build the adjacency matrix\n",
    "def build_adjacency_matrix(num_genes, interactions_file):\n",
    "    adjacency_matrix = np.zeros((num_genes, num_genes))\n",
    "    with open(interactions_file, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(',')\n",
    "            gene = int(float(tokens[0])) - 1  # Adjusting for zero-based indexing\n",
    "            num_targets = int(float(tokens[1]))\n",
    "            targets = [int(float(t)) - 1 for t in tokens[2:2 + num_targets]]\n",
    "            for target in targets:\n",
    "                adjacency_matrix[gene, target] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "def knn_imputation(ds1, n_neighbors=5):\n",
    "    # Replace zeros with NaN to mark missing values\n",
    "    ds1 = ds1.copy()\n",
    "    ds1[ds1 == 0] = np.nan\n",
    "    \n",
    "    # Initialize the imputed dataset\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    \n",
    "    # Number of genes and cells\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    \n",
    "    # Number of cell types (assuming 300 cells per type)\n",
    "    cells_per_type = 300\n",
    "    num_cell_types = num_cells // cells_per_type\n",
    "    \n",
    "    # Loop over each cell type\n",
    "    for i in range(num_cell_types):\n",
    "        start_idx = i * cells_per_type\n",
    "        end_idx = start_idx + cells_per_type\n",
    "        ds1_cell_type = ds1[:, start_idx:end_idx]\n",
    "        \n",
    "        # Transpose the data to shape (cells, genes) for KNNImputer\n",
    "        ds1_cell_type_T = ds1_cell_type.T\n",
    "        \n",
    "        # Initialize KNNImputer\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors, weights='distance')\n",
    "        \n",
    "        # Perform imputation\n",
    "        ds1_cell_type_imputed_T = imputer.fit_transform(ds1_cell_type_T)\n",
    "        \n",
    "        # Transpose back to original shape\n",
    "        ds1_cell_type_imputed = ds1_cell_type_imputed_T.T\n",
    "        \n",
    "        # Update the imputed dataset\n",
    "        ds1_imputed[:, start_idx:end_idx] = ds1_cell_type_imputed\n",
    "    \n",
    "    # Replace any remaining NaN values with zero\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed)\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    \n",
    "    return ds1_imputed\n",
    "\n",
    "def iterative_imputation(ds1):\n",
    "    ds1 = ds1.copy()\n",
    "    ds1[ds1 == 0] = np.nan\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    cells_per_type = 300\n",
    "    num_cell_types = num_cells // cells_per_type\n",
    "\n",
    "    for i in range(num_cell_types):\n",
    "        start_idx = i * cells_per_type\n",
    "        end_idx = start_idx + cells_per_type\n",
    "        ds1_cell_type = ds1[:, start_idx:end_idx]\n",
    "        ds1_cell_type_T = ds1_cell_type.T\n",
    "        imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "        ds1_cell_type_imputed_T = imputer.fit_transform(ds1_cell_type_T)\n",
    "        ds1_cell_type_imputed = ds1_cell_type_imputed_T.T\n",
    "        ds1_imputed[:, start_idx:end_idx] = ds1_cell_type_imputed\n",
    "\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed)\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    return ds1_imputed\n",
    "\n",
    "def deep_learning_imputation(ds1):\n",
    "    ds1 = ds1.copy()\n",
    "    ds1[ds1 == 0] = np.nan\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    num_cell_types = num_cells // 300  # Adjust cells per type if necessary\n",
    "\n",
    "    for i in range(num_cell_types):\n",
    "        start_idx = i * 300\n",
    "        end_idx = start_idx + 300\n",
    "        ds1_cell_type = ds1[:, start_idx:end_idx]\n",
    "        ds1_cell_type_T = ds1_cell_type.T\n",
    "        df = pd.DataFrame(ds1_cell_type_T)\n",
    "        \n",
    "        # Replace NaNs with zeros for DeepImpute\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        model = MultiNet()\n",
    "        model.fit(df)\n",
    "        imputed_data = model.predict(df)\n",
    "        ds1_cell_type_imputed = imputed_data.to_numpy().T\n",
    "        ds1_imputed[:, start_idx:end_idx] = ds1_cell_type_imputed\n",
    "\n",
    "    # Replace any NaNs or infs with zeros\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    return ds1_imputed\n",
    "\n",
    "def graph_convolutional_imputation(ds1, adjacency_matrix, num_epochs=100, learning_rate=0.01):\n",
    "    ds1 = ds1.copy()\n",
    "    ds1[ds1 == 0] = np.nan\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    \n",
    "    # Convert adjacency matrix to edge index\n",
    "    edge_index = np.array(adjacency_matrix.nonzero())\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    # Since we need to impute gene expressions, the nodes are genes, and features are cells\n",
    "    x = torch.tensor(ds1_imputed, dtype=torch.float)  # Shape: (num_genes, num_cells)\n",
    "    \n",
    "    class GCN(torch.nn.Module):\n",
    "        def __init__(self, num_features, hidden_channels):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "            self.conv2 = GCNConv(hidden_channels, num_features)\n",
    "    \n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "    \n",
    "    # Adjust the number of features and hidden channels\n",
    "    num_features = num_cells  # Features are cells\n",
    "    hidden_channels = 64\n",
    "    \n",
    "    model = GCN(num_features, hidden_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, edge_index)\n",
    "        mask = torch.isnan(x)\n",
    "        loss = loss_fn(output[~mask], x[~mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Impute missing values\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        imputed = model(x, edge_index)\n",
    "        x[mask] = imputed[mask]\n",
    "    ds1_imputed = x.numpy()\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    return ds1_imputed\n",
    "\n",
    "def graph_diffusion_imputation(ds1, adjacency_matrix, alpha=0.5, max_iter=100):\n",
    "    ds1 = ds1.copy()\n",
    "    ds1[ds1 == 0] = np.nan\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    num_genes, num_cells = ds1.shape\n",
    "\n",
    "    degrees = np.sum(adjacency_matrix, axis=1)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        D_inv = np.diag(1.0 / degrees)\n",
    "        D_inv[np.isinf(D_inv)] = 0.0\n",
    "\n",
    "    P = D_inv.dot(adjacency_matrix)\n",
    "    P = np.nan_to_num(P)\n",
    "    I = np.eye(num_genes)\n",
    "    epsilon = 1e-5\n",
    "    A = I - alpha * P + epsilon * np.eye(num_genes)\n",
    "\n",
    "    for cell_idx in range(num_cells):\n",
    "        y = ds1_imputed[:, cell_idx]\n",
    "        missing_indices = np.isnan(y)\n",
    "        if np.any(missing_indices):\n",
    "            x0 = np.zeros(num_genes)\n",
    "            y_filled = np.nan_to_num(y)\n",
    "            x, info = bicgstab(A, y_filled, x0=x0, maxiter=max_iter)\n",
    "            if info != 0:\n",
    "                print(f\"Warning: BiCGSTAB did not converge for cell {cell_idx}, info: {info}\")\n",
    "            y[missing_indices] = x[missing_indices]\n",
    "            ds1_imputed[:, cell_idx] = y\n",
    "\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed)\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    return ds1_imputed\n",
    "\n",
    "def saucie_imputation(ds1):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "\n",
    "    ds1 = ds1.copy()\n",
    "    # ds1[ds1 == 0] = np.nan\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    \n",
    "    # Number of cell types (assuming 300 cells per type)\n",
    "    cells_per_type = 300\n",
    "    num_cell_types = num_cells // cells_per_type\n",
    "\n",
    "    for i in range(num_cell_types):\n",
    "        start_idx = i * cells_per_type\n",
    "        end_idx = start_idx + cells_per_type\n",
    "        ds1_cell_type = ds1[:, start_idx:end_idx]\n",
    "        ds1_cell_type_T = ds1_cell_type.T\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        saucie = SAUCIE(ds1_cell_type_T.shape[1])\n",
    "        loadtrain = Loader(ds1_cell_type_T, shuffle=True)\n",
    "        saucie.train(loadtrain, steps=1000)\n",
    "        loadeval = Loader(ds1_cell_type_T, shuffle=False)\n",
    "        rec_ds1_T = saucie.get_reconstruction(loadeval)\n",
    "        rec_ds1 = rec_ds1_T.T\n",
    "        ds1_imputed[:, start_idx:end_idx] = rec_ds1\n",
    "\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed)\n",
    "    return ds1_imputed\n",
    "\n",
    "def magic_imputation(ds1):\n",
    "    ds1 = ds1.copy()\n",
    "    # ds1[ds1 == 0] = np.nan\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    \n",
    "    # Number of cell types (assuming 300 cells per type)\n",
    "    cells_per_type = 300\n",
    "    num_cell_types = num_cells // cells_per_type\n",
    "\n",
    "    for i in range(num_cell_types):\n",
    "        start_idx = i * cells_per_type\n",
    "        end_idx = start_idx + cells_per_type\n",
    "        ds1_cell_type = ds1[:, start_idx:end_idx]\n",
    "        ds1_cell_type_T = ds1_cell_type.T\n",
    "\n",
    "        ds1_filtered_T = scprep.filter.filter_rare_genes(ds1_cell_type_T, min_cells=5)\n",
    "        ds1_normalized_T = scprep.normalize.library_size_normalize(ds1_filtered_T)\n",
    "        ds1_sqrt_T = scprep.transform.sqrt(ds1_normalized_T)\n",
    "        magic_operator = magic.MAGIC(\n",
    "            # knn=5,\n",
    "            # knn_max=None,\n",
    "            # decay=1,\n",
    "            # Variable changed in paper\n",
    "            t='auto',\n",
    "            n_pca=20,\n",
    "            # solver=\"exact\",\n",
    "            # knn_dist=\"euclidean\",\n",
    "            n_jobs=-1,\n",
    "            # random_state=None,\n",
    "            # verbose=1,\n",
    "        )\n",
    "        ds1_imputed_T = magic_operator.fit_transform(ds1_sqrt_T)\n",
    "        ds1_imputed_cell_type = ds1_imputed_T.T\n",
    "\n",
    "        ds1_imputed[:, start_idx:end_idx] = ds1_imputed_cell_type\n",
    "\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed)\n",
    "    return ds1_imputed\n",
    "\n",
    "def scscope_imputation(ds1):\n",
    "    ds1 = ds1.copy()\n",
    "    ds1[ds1 == 0] = np.nan\n",
    "    num_genes, num_cells = ds1.shape\n",
    "    ds1_imputed = np.copy(ds1)\n",
    "    \n",
    "    # Number of cell types (assuming 300 cells per type)\n",
    "    cells_per_type = 300\n",
    "    num_cell_types = num_cells // cells_per_type\n",
    "\n",
    "    for i in range(num_cell_types):\n",
    "        start_idx = i * cells_per_type\n",
    "        end_idx = start_idx + cells_per_type\n",
    "        ds1_cell_type = ds1[:, start_idx:end_idx]\n",
    "        ds1_cell_type_T = ds1_cell_type.T\n",
    "        DI_model = scScope.train(\n",
    "            ds1_cell_type_T,\n",
    "            15,\n",
    "            use_mask=True,\n",
    "            batch_size=64,\n",
    "            max_epoch=1000,\n",
    "            epoch_per_check=100,\n",
    "            T=2,\n",
    "            exp_batch_idx_input=[],\n",
    "            encoder_layers=[],\n",
    "            decoder_layers=[],\n",
    "            learning_rate=0.0001,\n",
    "            beta1=0.05,\n",
    "            num_gpus=1)\n",
    "        _, rec_ds1_cell_type_T, _ = scScope.predict(ds1_cell_type_T, DI_model)\n",
    "        rec_ds1_cell_type = rec_ds1_cell_type_T.T\n",
    "        ds1_imputed[:, start_idx:end_idx] = rec_ds1_cell_type\n",
    "\n",
    "    ds1_imputed[ds1_imputed < 0] = 0.0\n",
    "    ds1_imputed = np.nan_to_num(ds1_imputed)\n",
    "    return ds1_imputed\n",
    "\n",
    "def run_pipeline(imputation_method, method_name, ds1_noisy, ds1_clean, gt, adjacency_matrix=None):\n",
    "    print(f\"Running {method_name}...\")\n",
    "    if 'Graph' in method_name:\n",
    "        ds1_imputed = imputation_method(ds1_noisy, adjacency_matrix)\n",
    "    else:\n",
    "        ds1_imputed = imputation_method(ds1_noisy)\n",
    "\n",
    "    # Evaluate imputation quality\n",
    "    mse = mean_squared_error(ds1_clean.flatten(), ds1_imputed.flatten())\n",
    "    print(f\"MSE after {method_name}: {mse:.4f}\")\n",
    "\n",
    "    # Proceed with GENIE3 and ROC AUC evaluation\n",
    "    ds1_imputed_T = ds1_imputed.T\n",
    "    VIM_imputed = GENIE3(ds1_imputed_T, nthreads=80, ntrees=100, regulators='all',\n",
    "                         gene_names=[str(s) for s in range(ds1_imputed_T.shape[1])])\n",
    "    roc_auc = roc_auc_score(gt.flatten(), VIM_imputed.flatten())\n",
    "    print(f\"ROC AUC Score after {method_name}: {roc_auc:.4f}\\n\")\n",
    "    return roc_auc, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Clean Data...\n",
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 80 threads\n",
      "Elapsed time: 117.49 seconds\n",
      "ROC AUC Score for Clean Data: 0.6816\n",
      "\n",
      "Evaluating Noisy Data...\n",
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 80 threads\n",
      "Elapsed time: 24.34 seconds\n",
      "ROC AUC Score for Noisy Data: 0.4306\n",
      "\n",
      "MSE between Noisy Data and Clean Data: 126.0642\n",
      "\n",
      "Summary of ROC AUC Scores:\n",
      "Clean Data: 0.6816\n",
      "Noisy Data: 0.4306\n",
      "\n",
      "Summary of MSE Values:\n",
      "Noisy Data: 126.0642\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ds1_clean, ds1_noisy = load_data()\n",
    "num_genes = ds1_noisy.shape[0]\n",
    "target_file = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/Interaction_cID_4.txt'\n",
    "gt = load_ground_truth(target_file, num_genes)\n",
    "adjacency_matrix = build_adjacency_matrix(num_genes, target_file)\n",
    "\n",
    "# Evaluate clean data\n",
    "print(\"Evaluating Clean Data...\")\n",
    "ds1_clean_T = ds1_clean.T\n",
    "VIM_clean = GENIE3(ds1_clean_T, nthreads=80, ntrees=100, regulators='all',\n",
    "                    gene_names=[str(s) for s in range(ds1_clean_T.shape[1])])\n",
    "roc_auc_clean = roc_auc_score(gt.flatten(), VIM_clean.flatten())\n",
    "print(f\"ROC AUC Score for Clean Data: {roc_auc_clean:.4f}\\n\")\n",
    "\n",
    "# Evaluate noisy data\n",
    "print(\"Evaluating Noisy Data...\")\n",
    "ds1_noisy_T = ds1_noisy.T\n",
    "VIM_noisy = GENIE3(ds1_noisy_T, nthreads=80, ntrees=100, regulators='all',\n",
    "                    gene_names=[str(s) for s in range(ds1_noisy_T.shape[1])])\n",
    "roc_auc_noisy = roc_auc_score(gt.flatten(), VIM_noisy.flatten())\n",
    "print(f\"ROC AUC Score for Noisy Data: {roc_auc_noisy:.4f}\\n\")\n",
    "\n",
    "# Compute MSE between noisy data and clean data\n",
    "mse_noisy = mean_squared_error(ds1_clean.flatten(), ds1_noisy.flatten())\n",
    "print(f\"MSE between Noisy Data and Clean Data: {mse_noisy:.4f}\\n\")\n",
    "\n",
    "# Define imputation methods\n",
    "methods = {\n",
    "    'KNN Imputation': knn_imputation,\n",
    "    'Iterative Imputation': iterative_imputation,\n",
    "    'DeepImpute': deep_learning_imputation,\n",
    "    'Graph Convolutional Network Imputation': graph_convolutional_imputation,\n",
    "    'Graph Diffusion Imputation': graph_diffusion_imputation,\n",
    "    'SAUCIE': saucie_imputation,\n",
    "    'MAGIC': magic_imputation,\n",
    "    'scScope': scscope_imputation\n",
    "}\n",
    "\n",
    "# Run pipeline for each method\n",
    "results = {}\n",
    "mse_results = {}\n",
    "for method_name, method_func in methods.items():\n",
    "    try:\n",
    "        if 'Graph' in method_name:\n",
    "            roc_auc, mse = run_pipeline(method_func, method_name, ds1_noisy, ds1_clean, gt, adjacency_matrix)\n",
    "        else:\n",
    "            roc_auc, mse = run_pipeline(method_func, method_name, ds1_noisy, ds1_clean, gt)\n",
    "        results[method_name] = roc_auc\n",
    "        mse_results[method_name] = mse\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {method_name}: {e}\\n\")\n",
    "\n",
    "# Print all results\n",
    "print(\"Summary of ROC AUC Scores:\")\n",
    "print(f\"Clean Data: {roc_auc_clean:.4f}\")\n",
    "print(f\"Noisy Data: {roc_auc_noisy:.4f}\")\n",
    "for method_name, roc_auc in results.items():\n",
    "    print(f\"{method_name}: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nSummary of MSE Values:\")\n",
    "print(f\"Noisy Data: {mse_noisy:.4f}\")\n",
    "for method_name, mse in mse_results.items():\n",
    "    print(f\"{method_name}: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
